# [2ì£¼ì°¨] ì‹¬í™”ê³¼ì œ: Multi-head Attentionìœ¼ë¡œ ê°ì • ë¶„ì„ ëª¨ë¸ êµ¬í˜„í•˜ê¸°

<aside>
ğŸ’¡ **ì´ë²ˆ ê³¼ì œì—ì„œëŠ” Transformer encoderì˜ ì™„ì „í•œ í˜•íƒœë¥¼ êµ¬í˜„í•©ë‹ˆë‹¤. Self-attentionì„ multi-head attentionìœ¼ë¡œ í™•ì¥í•˜ê³  layer normalization, dropout, residual connection ë“±ì˜ techniqueì„ ì ìš©í•˜ì—¬ ê°ì • ë¶„ì„ ì„±ëŠ¥ì„ í™•ì¸í•´ë´…ë‹ˆë‹¤.**

</aside>

## ì¤€ë¹„

---

Transformer ì‹¤ìŠµì„ ì§„í–‰í•œ notebook ìœ„ì—ì„œ ì§„í–‰í•´ì£¼ì‹œë©´ ë©ë‹ˆë‹¤:

[](https://drive.google.com/file/d/1vh4ALpRVicq9hdonHYiLTlsPhY6LLZAE/view?usp=share_link)

## ëª©í‘œ

---

- [ ]  Multi-head attention(MHA) êµ¬í˜„
    - Self-attention moduleì„ MHAë¡œ í™•ì¥í•´ì£¼ì‹œë©´ ë©ë‹ˆë‹¤. ì—¬ê¸°ì„œ MHAëŠ” ë‹¤ìŒê³¼ ê°™ì´ êµ¬í˜„í•©ë‹ˆë‹¤.
        1. ê¸°ì¡´ì˜ $W_q, W_k, W_v$ë¥¼ ì‚¬ìš©í•˜ì—¬ $Q, K, V$ë¥¼ ìƒì„±í•©ë‹ˆë‹¤. ì´ ë¶€ë¶„ì€ ì½”ë“œ ìˆ˜ì •ì´ í•„ìš” ì—†ìŠµë‹ˆë‹¤.
        2. $Q, K, V \in \mathbb{R}^{S \times D}$ê°€ ìˆì„ ë•Œ, ì´ë¥¼ $Q, K, V \in \mathbb{R}^{S \times H \times Dâ€™}$ìœ¼ë¡œ reshape í•´ì¤ë‹ˆë‹¤. ì—¬ê¸°ì„œ $H$ëŠ” `n_heads`ë¼ëŠ” ì¸ìë¡œ ë°›ì•„ì•¼ í•˜ê³ , $D$ê°€ $H$ë¡œ ë‚˜ëˆ  ë–¨ì–´ì§€ëŠ” ê°’ì´ì—¬ì•¼ í•˜ëŠ” ì œì•½ ì¡°ê±´ì´ í•„ìš”í•©ë‹ˆë‹¤. $D = H \times Dâ€™$ì…ë‹ˆë‹¤.
        3. $Q, K, V$ë¥¼ $Q, K, V \in \mathbb{R}^{H \times S \times Dâ€™}$ì˜ shapeìœ¼ë¡œ transposeí•´ì¤ë‹ˆë‹¤.
        4. $A = QK^T/\sqrt{D'} \in \mathbb{R}^{H \times S \times S}$ë¥¼ ê¸°ì¡´ì˜ self-attentionê³¼ ë˜‘ê°™ì´ ê³„ì‚°í•©ë‹ˆë‹¤. ì´ ë¶€ë¶„ì€ ì½”ë“œ ìˆ˜ì •ì´ í•„ìš” ì—†ìŠµë‹ˆë‹¤.
        5. Maskë¥¼ ë”í•©ë‹ˆë‹¤. ê¸°ì¡´ê³¼ $A$ì˜ shapeì´ ë‹¬ë¼ì¡Œê¸° ë•Œë¬¸ì— dimensionì„ ì–´ë–»ê²Œ ë§ì¶°ì¤˜ì•¼í• ì§€ ìƒê°í•´ì¤˜ì•¼ í•©ë‹ˆë‹¤.
        6. $\hat{x} = \textrm{Softmax}(A)V \in \mathbb{R}^{H \times S \times D'}$ë¥¼ ê³„ì‚°í•´ì£¼ê³  transposeì™€ reshapeì„ í†µí•´ $\hat{x} \in \mathbb{R}^{S \times D}$ì˜ shapeìœ¼ë¡œ ë‹¤ì‹œ ë§Œë“¤ì–´ì¤ë‹ˆë‹¤.
        7. ê¸°ì¡´ê³¼ ë˜‘ê°™ì´ $\hat{x} = \hat{x} W_o$ë¥¼ ê³±í•´ì¤˜ì„œ ë§ˆë¬´ë¦¬ í•´ì¤ë‹ˆë‹¤. ì´ ë˜í•œ ì½”ë“œ ìˆ˜ì •ì´ í•„ìš” ì—†ìŠµë‹ˆë‹¤.
- [ ]  Layer normalization, dropout, residual connection êµ¬í˜„
    - ë‹¤ì‹œ `TransformerLayer` classë¡œ ëŒì•„ì™€ì„œ ê³¼ì œë¥¼ ì§„í–‰í•˜ì‹œë©´ ë©ë‹ˆë‹¤.
    - Attention moduleì„ $MHA$, feed-forward layerë¥¼ $FFN$ì´ë¼ê³  í•˜ê² ìŠµë‹ˆë‹¤.
    - ê¸°ì¡´ì˜ êµ¬í˜„ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:
        
        ```python
        # x, mask is given
        
        x1 = MHA(x, mask)
        x2 = FFN(x1)
        
        return x2
        ```
        
    - ë‹¤ìŒê³¼ ê°™ì´ ìˆ˜ì •í•´ì£¼ì‹œë©´ ë©ë‹ˆë‹¤.
        
        ```python
        # x, mask is given
        
        x1 = MHA(x, mask)
        x1 = Dropout(x1)
        x1 = LayerNormalization(x1 + x)
        
        x2 = FFN(x1)
        x2 = Dropout(x2)
        x2 = LayerNormalization(x2 + x1)
        
        return x2
        ```
        
    - ì—¬ê¸°ì„œ `x1 + x`ì™€ `x2 + x1`ì— í•´ë‹¹í•˜ëŠ” ë¶€ë¶„ë“¤ì€ residual connectionì´ë¼ê³  ë¶€ë¦…ë‹ˆë‹¤.
- [ ]  5-layer 4-head Transformer
    - ê¸°ì¡´ ì‹¤ìŠµì—ì„œ ì‚¬ìš©í•œ hyper-parameterë“¤ê³¼ ìœ„ì—ì„œ êµ¬í˜„í•œ Transformerë¥¼ ê°€ì§€ê³  5-layer 4-head Transformerì˜ ì„±ëŠ¥ ê²°ê³¼ë¥¼ reportí•´ì£¼ì‹œë©´ ë©ë‹ˆë‹¤.
